\documentclass[11pt, fleqn]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{url}
\usepackage{hyperref}
\usepackage{enumerate}

% some manual commands for using regular font in equations
\newcommand{\Var}{\textrm{Var}}
\newcommand{\Cov}{\textrm{Cov}}
\newcommand{\E}{\textrm{E}}
\newcommand{\se}{\textrm{se}}

\begin{document}
\setlength\parindent{0pt}

\begin{center}
\textbf{Lecture 10: Properties of Least Squares Estimates}\\
\textbf{STAT 632, Spring 2020}\\
\hrulefill
\end{center}

\textbf{Preliminaries}: Let $a$ and $b$ be constants, and $X$ a random variable.   
\begin{align*}
\E(aX + b) = \\
\Var(aX + b) = \\
\end{align*}

\textbf{Random Vectors}\\
Let $X_1, X_2, \cdots, X_n$ be random variables.  An $n \times 1$ random vector is given by\footnote{Note that the $\bm{X}$ here is different than the $n \times (p+1)$ model matrix in regression.}
\[
\bm{X} = 
\begin{pmatrix}
X_1 \\
X_2 \\
\vdots\\
X_n
\end{pmatrix}
\]

The \textbf{variance-covariance} matrix of $\bm{X}$ is the $n \times n$ symmetric matrix

\[
\Var(\bm{X}) =
\begin{pmatrix}
\Var(X_1) & \Cov(X_1, X_2) & \cdots & \Cov(X_1, X_n)\\
\Cov(X_2, X_1) & \Var(X_2) & \cdots & \Cov(X_2,X_n)\\
\vdots\\ 
\Cov(X_n, X_1) & \Cov(X_n, X_2) &  \cdots & \Var(X_n)
\end{pmatrix}
\]

Note that $\Cov(X_i, X_i) = Var(X_i)$.\\

\textbf{Properties of Expectation and Variance for Random Vectors}\\
Let $\bm{A}$ be an $m \times n$ matrix of constants, $\bm{b}$ an $m \times 1$ vector of constants, and $\bm{X}$ an $n \times 1$ random vector, then
\begin{align*}
\E(\bm{AX} + \bm{b}) = \bm{A} \E(\bm{X}) + \bm{b}\\
\Var(\bm{AX} + \bm{b}) = \bm{A} \Var(\bm{X}) \bm{A'}\\
\end{align*}
\clearpage

\textbf{Properties of the Multiple Linear Regression Model}\\
$$\bm{Y} = \bm{X}\bm{\beta} + \bm{e}$$
\begin{itemize}
\item $\bm{Y}$ is an $n \times 1$ response vector
\item $\bm{X}$ is an $n \times (p+1)$ design matrix for the predictors
\item $\bm{\beta}$  is a $(p+1) \times 1$ vector of regression parameters
\item $\bm{e}$ is an $n \times 1$ vector of random errors; assuming $e_i \sim N(0, \sigma^2)$, independently
\end{itemize}
The variance-covariance matrix for the random errors $\bm{e}$ is given by\\
\[
\Var(\bm{e}) = 
\begin{pmatrix}
\sigma^2 & 0 & \cdots & 0\\
0 & \sigma^2 & \cdots & 0\\
\vdots\\
0 & 0 & \cdots & \sigma^2
\end{pmatrix}
= \sigma^2 \bm{I}_n
\]\\
which follows since $\Var(e_i) = \sigma^2$, and $\Cov(e_i, e_j) = 0$ when $i \neq j$ (by independence).\\

Use the properties of expectation and variance for random vectors to find $\E(\bm{Y})$, $\Var(\bm{Y})$, and the distribution of $\bm{Y}$.\\
\vspace{5cm}

\textbf{Properties of Least Squares Estimates}\\
Recall that the least squares estimates of the population parameters $\bm{\beta}$ is given by the $(p+1) \times 1$ vector
\begin{align*}
\bm{\hat{\beta}} = (\bm{X'X})^{-1} \bm{X'Y}
\end{align*}
The estimator $\bm{\hat{\beta}}$ is a random vector since it varies from sample to sample.\\
\clearpage

Show that $E(\bm{\hat{\beta}}) = \bm{\beta}$, the least squares estimates are unbiased.\\
\vspace{5cm}

Show that that variance-covariance matrix for the least squares estimates is given by $\Var(\bm{\hat{\beta}}) = \sigma^2 (\bm{X'X})^{-1}$\\
\vspace{7cm}

The variance of a particular estimate is given by the diagonal entry:
\begin{align*}
\Var(\hat{\beta}_{j}) = \sigma^2 (\bm{X'X})^{-1}_{j+1, j+1} 
\quad \text{for } j=0,\cdots,p
\end{align*}
This is how standard errors are calculated when constructing confidence intervals for the parameters.  For example, a $1-\alpha$ confidence interval for $\hat{\beta}_j$ is given by
\begin{align*}
\hat{\beta}_j \pm t_{\alpha /2; n-p-1} \se(\hat{\beta}_j)
\end{align*}
where $\se(\hat{\beta}_j) = \hat{\sigma} \sqrt{(\bm{X'X})^{-1}_{j+1,j+1}}$, and $\hat{\sigma} = \sqrt{\text{RSS} / (n-p-1)}$

\clearpage

Using the menu pricing data set (lecture 7), the code below demonstrates how to manually compute the least squares estimates, variance-covariance matrix, and standard errors.  The results are compared to the output from \texttt{lm()}.  Note that it is always better to use \texttt{lm()} than to do the computations manually.  I am just showing this to verify all the formulas.

<<>>=
nyc <- read.csv("https://ericwfox.github.io/data/nyc.csv")

# response vector
Y <- matrix(nyc$Price, ncol=1)

# design matrix
X <- cbind(Intercept = 1, nyc[,c('Food', 'Decor', 'East')])
X <- as.matrix(X)
rownames(X) <- nyc$Restaurant
X[1:5,]

# manually calculate least squares estimates
betaHat <- solve(t(X) %*% X) %*% t(X) %*% Y
betaHat

# compare with lm()
lm1 <- lm(Price ~ Food + Decor + East, data=nyc)
coef(lm1)
@
\clearpage

<<>>=
# manually calculate standard errors for least squares estimates
n <- nrow(nyc)
p <- 3
resid <- as.numeric(Y - X %*% betaHat)
sigmaHat2 <- sum(resid^2) / (n-p-1)
covBetaHat <- sigmaHat2 * solve(t(X) %*% X)
covBetaHat
seBetaHat <- sqrt(diag(covBetaHat))
seBetaHat

# compare with lm()
summary(lm1)$coef
vcov(lm1)
@


\end{document}